\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Motivation}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Novelty}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminary}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}A2C}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Noise Network}{1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Graphical representation of a noisy linear layer. The parameters $\mu _w$ , $\mu _b$ , $\sigma _w$ and $\sigma _b$ are the learnables of the network whereas $\epsilon _w$ and $\epsilon _b$ are noise variables which can be chosen in factorised or non-factorised fashion. The noisy layer functions similarly to the standard fully connected linear layer. The main difference is that in the noisy layer both the weights vector and the bias is perturbed by some parametric zero-mean noise, that is, the noisy weights and the noisy bias can be expressed as $w = \mu ^w+\sigma ^w \odot \epsilon ^w$ and $b = \mu ^b + \sigma ^b \odot \epsilon ^b$, respectively. The output of the noisy layer is then simply obtained as $y = wx + b$.}}{2}\protected@file@percent }
\newlabel{fig:enter-label}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Random Encoders for Efficient Exploration}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Intrinsic Curiosity Method}{2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The structure of ICM}}{2}\protected@file@percent }
\newlabel{fig:enter-label}{{2}{2}}
\bibstyle{IEEEtran}
\bibdata{IEEEabrv,reference}
\bibcite{c1}{1}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The back propagation of ICM.}}{3}\protected@file@percent }
\newlabel{fig:enter-label}{{3}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Dropout for exploration}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Implementation details}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Observation space and stating state}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Action space}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Extrinsic Reward setting}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiment}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Others}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}MPC and PID}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Survey of Imitation Learning}{4}\protected@file@percent }
